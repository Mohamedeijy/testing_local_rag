Reference: [[Modern Computer Vision with PyTorch]]
Subjects: [[Deep Learning]]
Tags: #neuralnetwork

Starting with the loss generated from the feedforward propagation, we update the weights of the network in such a way that the loss value is minimized as much as possible.
This is done through:
1. Change each weigh within the nn by a small amount - one at a time.
2. Measure the change in loss (δL ) when the weight value is changed (δW ).
3. Update the weight by -k.(δL/δW) where k is a positive value and is a 
hyperparameter known as the learning rate.

Executing this process for a neural network is tasking and could take many little changes of each weight if done to a network from thousands to billions of weights. Calculus teaches us that measuring a change of a function (here our loss function) based on a weight change, can be done by calculating the **gradient** of the function concerning the weight.

--- 
##### Example with gradient from slight value change

Before using partial derivatives from calculus, we'll start by calculating gradient descent from scratch by tweaking each weight, which will help us understand this process better and also grasp how important the **learning rate** is.

The book provides a simple code going through this process:

```
def update_weights(inputs, outputs, weights, lr):
    original_weights = deepcopy(weights)
    temp_weights = deepcopy(weights)
    updated_weights = deepcopy(weights)
    original_loss = feed_forward(inputs, outputs, original_weights)
    for i, layer in enumerate(original_weights):
        for index, weight in np.ndenumerate(layer):
            temp_weights = deepcopy(weights)
            temp_weights[i][index] += 0.0001
            _loss_plus = feed_forward(inputs, outputs, temp_weights)
            grad = (_loss_plus - original_loss)/(0.0001)
            updated_weights[i][index] -= grad*lr
    return updated_weights, original_loss
```

Looping though each layer, then each weight of the network, the loss is generated by for the weight slightly tweaked by 0.0001 (this is done to mimic the differentiation calculation in calculus, the difference created by a slight shift in the value of a variable) and then this loss and the original loss are used to calculate a gradient based on the weight. We then subtract to the weight this gradient in proportion to the **learning rate**. 

Here we computed gradients using every data point available at once. This is in impractical with increasing network size:

	However, in practice, when we have thousands (or in some cases, millions) of data
	points, the incremental contribution of a greater number of data points while
	calculating the loss value would follow the law of diminishing returns, and hence we
	would be using a batch size that is much smaller compared to the total number of data
	points we have.
In practice we need a smaller [[Batch size]] than just the total number of data points. And we do gradient descent for one batch at a time until we've exhausted all data points, within one **epoch** of training.

	The typical batch size considered in building a model is anywhere between 32 and 1,024.

---
##### Example with gradient by leveraging the chain rule

	One drawback of updating weight values in this manner is that when the network is
	large, a large number of computations are needed to calculate loss values (and in fact,
	the computations are to be done twice – once where weight values are unchanged and
	again where weight values are updated by a small amount).

The chain rule allows us to get the gradient of the loss concerning a weight value without having to manually calculate loss values. 

To demonstrate the chain rule, let's visualize the network (it will only be applied the weight w11 for the example):

![[Pasted image 20230901145320.png]]

The impact of the change in the loss value (C) with respect to the change in weight as follows:
![[Pasted image 20230901150427.png]]

	Essentially, we are performing a chain of differentiations to fetch the differentiation
	of our interest.


All the partial derivatives are broken down as follows:
- Loss value with respect to predicted value ŷ:
	![[Pasted image 20230901151101.png]]

- Predicted value ŷ in respect to hidden layer activation value a11: 
	![[Pasted image 20230901151201.png]]

- hidden layer activation value a11 with respect to the hidden layer value prior to activation h11 (derivative of sigmoid a is a*(1-a) ):
	![[Pasted image 20230901151243.png]]


- hidden layer value prior to activation h11 with respect to the weight value w11:
	![[Pasted image 20230901151422.png]]

	Replacing each of the partial differentiation terms with the corresponding value as calculated in the previous steps, we get:
![[Pasted image 20230901151508.png]]

```
From the preceding formula, we can see that we are now able to calculate the impact on the loss value of a small change in the weight value (the gradient of the loss with respect to weight) without brute-forcing our way by recomputing the feedforward propagation again. Next, we will go ahead and update the weight value as follows:

updated weight = original weight — learning rate * Gradient of loss with respect to weight
```

---


[[Stochastic gradient descent]] is the general method used to do this minimizing task. Apart from stochastic gradient descent, many other similar optimizers help to minimize loss values, they will be catalogued in [[Optimization Algorithms in Deep Learning]].
