---
References: "[[Modern Computer Vision with PyTorch]]"
Subjects: "[[Deep Learning]]"
tags:
  - computervision
  - objectdetection
  - cnn
  - rcnn
---

# Basics of Object Detection 

Training a typical object detection model involves the following steps: 

1. Creating ground truth data that contains labels of the bounding box and class corresponding to various objects present in the image. 

2. Coming up with mechanisms that scan through the image to identify regions (region proposals) that are likely to contain objects. In this chapter, we will learn about leveraging region proposals generated by a method named **selective search**. In the next chapter, we will learn about leveraging anchor boxes to identify regions containing objects. In the chapter on combining computer vision and NLP techniques (Chapter 15), we will learn about leveraging positional embeddings in transformers to aid in identifying the regions containing an object. 

3. Creating the target class variable by using the IoU metric.

4. Creating the target bounding box offset variable to make corrections to the location of region proposal coming in the second step. 

5. Building a model that can predict the class of object along with the target bounding box offset corresponding to the region proposal. 

6. Measuring the accuracy of object detection using mean Average Precision (mAP).

## Creating a bounding box ground truth for training

To build the algorithm that predicts the bounding box around an object, we have to create the input-output combinations. The input being an image and the output being bounding boxes around each object in the image. 

Thus the data points fed to our model should be constituted by our images and the bounding coordinates of every object inside our image (along with its corresponding class if needed, these two elements are usually stored within an XML file upon creation of the dataset).
In the book, the tool `ybat` is used to annotate the bounding boxes. Other tools are referenced in a video I saw on the YouTube channel "DigitalSreeni" ("10 best annotation tools for computer vision applications"). 

To annotate a set of images using `ybat`, a `classes.txt` file is created with the names of the classes:
![[Pasted image 20231005150529.png]]

Each image can be labelled by selecting a set class and drawing a crosshair around the area of interest, and then saving the datadump in the desired format (YOLO, COCO, VOC):
![[Pasted image 20231005150720.png]]


## What are region proposals ?

```
Imagine a hypothetical scenario where the image of interest contains a person and sky in the background. Furthermore, for this scenario, let's assume that there is little change in pixel intensity of the background (sky) and that there is a considerable change in pixel intensity of the foreground (the person). Just from the preceding description itself, we can conclude that there are two primary regions here â€“ one is of the person and the other is of the sky. Furthermore, within the region of the image of a person, the pixels corresponding to hair will have a different intensity to the pixels corresponding to the face, establishing that there can be multiple sub-regions within a region.
```

Region proposal is the differentiation of areas within an image through pixel similarity.

### Generate a  region proposal with SelectiveSearch

SelectiveSearch is a region proposal algorithm for object localization, it uses pixel intensity to create regions of pixels that are likely to be grouped together.

```
SelectiveSearch groups pixels based on the hierarchical grouping of similar pixels, which, in turn, leverages the color, texture, size, and shape compatibility of content within an image. 

Initially, SelectiveSearch over-segments an image by grouping pixels based on the preceding attributes. 
Next, it iterates through these over-segmented groups and groups them based on similarity. At each iteration, it combines smaller regions to form a larger region.
```

Selectivesearch starts with over-segmentation based on the previously mentioned attributes. Here we have an example of segmentation results using felzenszwalb segmentation on a scale of 200:
![[Pasted image 20231005155247.png]]

Pixels with the same value are part of the same region proposal. This can now guide us through the bounding box creation.
![[Pasted image 20231005160302.png]]


In the book, we try and generate these boxes for our image, with the following result:
![[Pasted image 20231005161537.png]]

How do we leverage this technique for object detection and localization ?

We do so by establishing the fact that a region proposal with a high degree of intersection with our ground truth is one that contains the object. This is the intuition for **IoU (Intersection over Union)**.

## Intersection over Union (IoU)

IoU is the ratio between the overlapping of two areas over their union.

![[Pasted image 20231005162619.png]]

Following the variation of IoU through this graph can give us an intuition:
![[Pasted image 20231005162834.png]]


## Non-max suppression

This notion is of interest in scenarios in which we have multiple region proposals that significantly overlap one another, like in this image:

![[Pasted image 20231005163704.png]]

Non-max suppression refers to the practice of identifying the box with the highest probability of containing an object, and discarding any other one that has an IoU with the max box over a certain threshold.

```
In PyTorch, non-max suppression is performed using the nms function in the torchvision.ops module. The nms function takes the bounding box coordinates, the confidence of the object in the bounding box, and the threshold of IoU across bounding boxes, to identify the bounding boxes to be retained.
```

## Mean average precision

Before we try to understand mAP, let's first understand precision, then average precision, and finally, mAP:

- Precision:
![[Pasted image 20231005164004.png]]

- Average Precision: the average of the precisions calculated at different IoU thresholds.
- mAP: mean of the average precisions across all classes.


## [[R-CNN based custom object detectors]]

