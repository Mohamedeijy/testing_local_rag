{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Repurposing previous local RAG playground, integrating Pinecone."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### UTILS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing os module for environment variables\n",
    "import os\n",
    "# import pandas as pd\n",
    "# importing necessary functions from dotenv library\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Loading, Splitting\n",
    "Load our Obsidian markdown notes. We'll split them first by headings to maintain a sense of structure that could be used in our metadata."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "\n",
    "# can't split document with this splitter, we create a function that splits the content of a document into smaller docs using markdown header splitter\n",
    "def split_md_by_headings(doc: Document, markdown_splitter: MarkdownHeaderTextSplitter) -> list[Document] :\n",
    "    \"\"\" splits the content of a document into smaller docs using markdown header splitter\"\"\"\n",
    "    initial_metadata : dict = doc.metadata\n",
    "    header_splits : list[Document] = markdown_splitter.split_text(doc.page_content)\n",
    "    for doc in header_splits:\n",
    "        doc.metadata.update(initial_metadata)\n",
    "    return header_splits\n",
    "\n",
    "# get directory with md files\n",
    "MD_FILES_DIRECTORY : str = os.getenv(\"MD_FILES_DIRECTORY\")\n",
    "print(f\"Markdown files directory: {MD_FILES_DIRECTORY}\")\n",
    "\n",
    "# Markdown spliter\n",
    "headers_to_split_on : list[tuple] = [(\"#\", \"Header 1\"),(\"##\", \"Header 2\"),]\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "# we use a raw text loader instead of a markdown loader because that one strips the doc of all it's headings\n",
    "loader = DirectoryLoader(path=MD_FILES_DIRECTORY, glob=\"*.md\", loader_cls=TextLoader, show_progress=True)\n",
    "markdowns : list[Document] = loader.load()\n",
    "\n",
    "# split our docs by heading\n",
    "markdowns_split_headers : list[list[Document]] = [split_md_by_headings(doc, md_splitter) for doc in markdowns]\n",
    "markdowns_split_headers : list[Document] = list(flatten(markdowns_split_headers))\n",
    "\n",
    "for doc in markdowns_split_headers:\n",
    "    print(doc.dict()['metadata'], len(doc.page_content))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "79"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(markdowns_split_headers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We haven't really introduced token-limit concerns in the mix yet. We'll experiment with different chunk size limits. We use a special function again so we can preserve heading metadata (just in case). For now we use a character limit instead of a token limit."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 783, which is longer than the specified 512\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 526, which is longer than the specified 512\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 607, which is longer than the specified 512\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 520, which is longer than the specified 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of chunks: 290\n",
      "Number of chunks bigger than our 512 character limit: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def split_md_by_chunk(doc: Document, text_splitter: CharacterTextSplitter) -> list[Document] :\n",
    "    \"\"\" splits the content of a document into smaller docs using text splitter\"\"\"\n",
    "    initial_metadata : dict = doc.metadata\n",
    "    header_splits : list[Document] = text_splitter.create_documents([doc.page_content])\n",
    "    for doc in header_splits:\n",
    "        doc.metadata.update(initial_metadata)\n",
    "    return header_splits\n",
    "\n",
    "chunk_size : int = 512\n",
    "\n",
    "character_text_splitter = CharacterTextSplitter(\n",
    "    separator = \".\",\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = 20\n",
    ")\n",
    "\n",
    "# split our docs by chunk size\n",
    "markdowns_split_chunks : list[list[Document]] = [split_md_by_chunk(doc, character_text_splitter) for doc in markdowns_split_headers]\n",
    "markdowns_split_chunks : list[Document] = list(flatten(markdowns_split_chunks))\n",
    "print(f\"Final number of chunks: {len(markdowns_split_chunks)}\\n\"\n",
    "      f\"Number of chunks bigger than our {chunk_size} character limit: {len([_ for doc in markdowns_split_chunks if len(doc.page_content) > chunk_size ])}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate embeddings, store in dataframe\n",
    "We'll use NomicEmbed as a model, an open-source model showing good results on Massive Text Embedding Benchmark (MTEB) in comparison to other small embedding model like `text-embedding-3-small`. We login using `nomic login` on our terminal and set `NOMIC_API_KEY` as an environment variable. In the future, we'll try and exploit our own local LLaMa 3 as an embedding model, using \"[LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)\" as a basis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "dimensionality : int = 768\n",
    "embed_model = NomicEmbeddings(model=\"nomic-embed-text-v1.5\", dimensionality=dimensionality) # not quite clear yet what specific dimensionality I have to work with\n",
    "embed_model.embed_query(\"My query to look up\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# generate embedding from a text\n",
    "def get_embeddings(text: str) -> list[float]:\n",
    "    \"\"\" take a string and return embeddings in the form of a vector of floats\"\"\"\n",
    "    return embed_model.embed_query(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We streamline the generation of a dataframe based on our list of langchain documents."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def document_list_to_dataframe(docs: list[Document]) -> pd.DataFrame :\n",
    "    \"\"\" extract data from list of documents and store in dataframe ('id', 'embeddings', 'metadata'(text inside here)) \"\"\"\n",
    "    columns = ['embeddings', 'metadata']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for doc in docs:\n",
    "        # get document data\n",
    "        text : str = doc.page_content\n",
    "        doc_metadata : dict = doc.dict()['metadata']\n",
    "        # generate embeddings\n",
    "        embeds : list[float] = get_embeddings(text)\n",
    "        # insert text in metadata dict\n",
    "        doc_metadata.update({'text': text})\n",
    "        df = pd.concat([df, pd.DataFrame([[embeds,doc_metadata]], columns=columns)], ignore_index=True)\n",
    "\n",
    "    df.reset_index()\n",
    "    # df['id'] = range(1, len(df) + 1)\n",
    "    # df['id'] = str(df['id'])\n",
    "    # df.set_index('id', inplace=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def get_pinecone_dicts(df: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\" dataframe data into list of dicts to upsert in Pinecone index \"\"\"\n",
    "    dicts = df.to_dict(orient='records')\n",
    "\n",
    "    pinecone_dicts = []\n",
    "    for i, df_dict in enumerate(dicts):\n",
    "        pc_dict = {\n",
    "            'id': str(i),\n",
    "            'values': df_dict['embeddings'],\n",
    "            'metadata': df_dict['metadata']\n",
    "        }\n",
    "        pinecone_dicts.append(pc_dict)\n",
    "    return pinecone_dicts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          embeddings  \\\n0  [0.002105713, 0.01939392, -0.18383789, -0.0800...   \n1  [0.016067505, 0.020019531, -0.171875, -0.01194...   \n2  [0.026245117, 0.10760498, -0.17236328, -0.0895...   \n3  [0.018203735, 0.09442139, -0.1652832, -0.06201...   \n4  [0.029800415, 0.09063721, -0.18261719, -0.0787...   \n\n                                            metadata  \n0  {'source': 'data\\computer_science_notes\\Advanc...  \n1  {'Header 1': 'Components of modern object dete...  \n2  {'Header 1': 'Components of modern object dete...  \n3  {'Header 1': 'Components of modern object dete...  \n4  {'Header 1': 'Components of modern object dete...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>embeddings</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.002105713, 0.01939392, -0.18383789, -0.0800...</td>\n      <td>{'source': 'data\\computer_science_notes\\Advanc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.016067505, 0.020019531, -0.171875, -0.01194...</td>\n      <td>{'Header 1': 'Components of modern object dete...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.026245117, 0.10760498, -0.17236328, -0.0895...</td>\n      <td>{'Header 1': 'Components of modern object dete...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.018203735, 0.09442139, -0.1652832, -0.06201...</td>\n      <td>{'Header 1': 'Components of modern object dete...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0.029800415, 0.09063721, -0.18261719, -0.0787...</td>\n      <td>{'Header 1': 'Components of modern object dete...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = document_list_to_dataframe(markdowns_split_chunks[:int(len(markdowns_split_chunks)*0.7)])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "vector_dicts = get_pinecone_dicts(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting Pinecone index instance and execute data ingestion script."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "PINECONE_API_KEY : str = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pinecone import PineconeApiException\n",
    "\n",
    "index_name : str = \"markdown-notes\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=dimensionality, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to the index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "{'dimension': 768,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 1}},\n 'total_vector_count': 1}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index : Pinecone.Index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsert the data to Pinecone."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "{'upserted_count': 203}"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(vectors=vector_dicts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check our vector count."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{'dimension': 768,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 204}},\n 'total_vector_count': 204}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing index with a query."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "{'matches': [{'id': '45',\n              'metadata': {'source': 'data\\\\computer_science_notes\\\\Batch '\n                                     'size.md',\n                           'text': 'Advantages of using a batch size < number '\n                                   'of all samples:  \\n'\n                                   '- It requires less memory. Since you train '\n                                   'the network using fewer samples, the '\n                                   'overall training procedure requires less '\n                                   \"memory. That's especially important if you \"\n                                   'are not able to fit the whole dataset in '\n                                   \"your machine's memory.  \\n\"\n                                   '- Typically networks train faster with '\n                                   \"mini-batches. That's because we update the \"\n                                   'weights after each propagation'},\n              'score': 0.762337685,\n              'values': []},\n             {'id': '118',\n              'metadata': {'source': 'data\\\\computer_science_notes\\\\Implementing '\n                                     'backpropagation.md',\n                           'text': 'And we do gradient descent for one batch '\n                                   \"at a time until we've exhausted all data \"\n                                   'points, within one **epoch** of '\n                                   'training.  \\n'\n                                   'The typical batch size considered in '\n                                   'building a model is anywhere between 32 '\n                                   'and 1,024'},\n              'score': 0.749031067,\n              'values': []},\n             {'id': '43',\n              'metadata': {'source': 'data\\\\computer_science_notes\\\\Batch '\n                                     'size.md',\n                           'text': '[From user @itdxer in '\n                                   'stakoverflow.com]([python - What is batch '\n                                   'size in neural network? - Cross Validated '\n                                   '(stackexchange.com)](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)) '\n                                   ':\\n'\n                                   'Subjects: [[Deep Learning]]  \\n'\n                                   'TheÂ\\xa0**batch size**Â\\xa0defines the '\n                                   'number of samples that will be propagated '\n                                   'through the network.  \\n'\n                                   \"For instance, let's say you have 1050 \"\n                                   'training samples and you want to set up '\n                                   'aÂ\\xa0`batch_size`Â\\xa0equal to 100'},\n              'score': 0.74096179,\n              'values': []}],\n 'namespace': '',\n 'usage': {'read_units': 6}}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"batch size\"\n",
    "embedded_query = embed_model.embed_query(query)\n",
    "index.query(\n",
    "    vector=embedded_query,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create chain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll use our local LLaMa 3."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"llama3:8b\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "\"What a great question!\\n\\nThe short answer: The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. Here's what happens:\\n\\n1. **Sunlight**: When sunlight enters Earth's atmosphere, it contains all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\\n2. **Molecules**: The atmosphere is made up of tiny molecules of gases like nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light.\\n3. **Scattering**: When sunlight hits these molecules, it scatters in all directions. This scattering effect is more pronounced for shorter wavelengths (like blue and violet) than longer wavelengths (like red and orange).\\n4. **Blue dominance**: As a result of this scattering, the shorter wavelengths (blue and violet) are distributed throughout the sky, making it appear blue to our eyes.\\n\\nThe longer wavelengths (red and orange) continue to travel in a more direct path to our eyes, which is why we see them as the sun's color. The combination of scattered blue light and direct red-orange light gives us the blue sky we're familiar with!\\n\\nOther factors can influence the apparent color of the sky, such as:\\n\\n* **Atmospheric conditions**: Dust, pollution, and water vapor in the air can scatter light in different ways, changing the sky's color.\\n* **Time of day**: The angle of the sun and the amount of scattered light can affect the sky's color during sunrise and sunset.\\n* **Altitude and atmospheric pressure**: Changes in altitude or atmospheric pressure can alter the scattering patterns and the resulting sky color.\\n\\nNow, go outside and enjoy that beautiful blue sky!\""
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# test openai and llama message response\n",
    "if MODEL.startswith('gpt'):\n",
    "    model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL,\n",
    "                   keep_alive=1, # keep model loaded to gain time\n",
    "                   temperature=0,\n",
    "                   )\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3:8b\")\n",
    "\n",
    "model.invoke(\"Why is the sky blue ?\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll keep the parsing module and everything the same."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "\"The color of the sky can appear different depending on the time of day, atmospheric conditions, and the observer's location. However, under normal conditions, the sky typically appears blue to our eyes because of a phenomenon called Rayleigh scattering.\\n\\nRayleigh scattering is the scattering of light by small particles or molecules in the atmosphere, such as nitrogen (N2) and oxygen (O2). These gases are much smaller than the wavelength of visible light, so they scatter shorter (blue) wavelengths more efficiently than longer (red) wavelengths. This means that when sunlight enters Earth's atmosphere, the blue and violet colors are scattered in all directions by these tiny molecules, reaching our eyes from all parts of the sky.\\n\\nHere's a simplified explanation:\\n\\n1. Sunlight contains all the colors of the visible spectrum (ROY G BIV: red, orange, yellow, green, blue, indigo, and violet).\\n2. When sunlight enters the atmosphere, it encounters tiny molecules of gases like N2 and O2.\\n3. These molecules scatter the shorter wavelengths (blue and violet) more effectively than the longer wavelengths (red and orange).\\n4. As a result, the blue light is scattered in all directions, reaching our eyes from all parts of the sky.\\n5. The other colors (red, orange, yellow, green, indigo, and violet) continue to travel in a straight line, reaching our eyes only when they are not scattered by the atmosphere.\\n\\nThis scattering effect gives the sky its blue appearance during the daytime, especially near the horizon where the sunlight has to travel through more of the Earth's atmosphere. The color can appear more intense or deeper blue when the sun is overhead and the light has traveled a shorter distance through the atmosphere.\\n\\nKeep in mind that atmospheric conditions like pollution, dust, and water vapor can affect the apparent color of the sky, making it appear more hazy or gray. Additionally, during sunrise and sunset, the angle of the sunlight and the scattering effects can create a range of colors, from orange to pink to purple, depending on the specific conditions.\\n\\nSo, to summarize: the sky appears blue because of the scattering of shorter wavelengths (blue and violet) by tiny molecules in the atmosphere, which reaches our eyes from all parts of the sky.\""
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "# pipe the output of our model into the input of our parser\n",
    "chain = model | parser\n",
    "# instead of invoking the model, we invoke the chain instead\n",
    "chain.invoke(\"Why is the sky blue ?.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a prompt template encompassing the context to give the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a virtual assistant strictly designed to provide knowledge based on provided context from a database of documents.\n",
      "Answer the question based on the context below.\n",
      "If you can't answer the question, reply 'I don't know'.\n",
      "\n",
      "Context: Here is some context\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "You are a virtual assistant strictly designed to provide knowledge based on provided context from a database of documents.\n",
    "Answer the question based on the context below.\n",
    "If you can't answer the question, reply 'I don't know'.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To pass this prompt to our model, we expand upon our chain."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'PromptInput',\n 'type': 'object',\n 'properties': {'context': {'title': 'Context', 'type': 'string'},\n  'question': {'title': 'Question', 'type': 'string'}}}"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "'According to the context, Jimi Hendrix is still considered to be the best guitarist of all time.'"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to invoke a chain we need to understand the structure of our prompt template, which can be seen on the input schema above\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"In the 1960s, the rock scene was an effervescent field of talented and eccentric musicians brought up through the hippie movement. Jimi Hendrix, Eric Clapton, Jimmy Page are a few guitarists of that era who experienced a lot of success. Jimi Hendrix is still considered to be the best guitarist of all time.\",\n",
    "        \"question\": \"Who is the greatest guitarist ever ?\",\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make it so that our chain receives our documents' relevant information as context, we'll use our pinecone index. LangChain provides a Pinecone vectorstore instance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embed_model,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A retriever is a component of Langchain that allows to retrieve information from a vector store (can retrieve from other sources). Using invoke to retrieve the top k closest documents most relevant to the prompt inputted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='#### Statistical test\\n- Statistics is to help make decisions based on quantifiable uncertainties\\n- A hypothesis test contains a **null hypothesis** (no difference between data) and an **alternative hypothesis** (difference between data), difference based on a **critical value**, a benchmark\\n- A hypothesis test can test the following:\\n- One variable against another (such as in a t-test)\\n- Multiple variables against one variable (for example, linear regression)\\n- Multiple variables against multiple variables (for example, MANOVA)  \\n### Sampling strategies  \\nTwo types of sampling methods:  \\n#### Probability sampling  \\n- A sample is chosen based on a theory of probability, or randomly with random selection (every member has the same chance)  \\n4 types of probability sampling', metadata={'Header 1': 'Part 1, An introduction to statistics', 'Header 2': 'Chapter 1, Sampling and Generalization', 'source': 'data\\\\computer_science_notes\\\\Building Statistical Models in Python.md'}),\n Document(page_content='Permutation testing is used to test the null hypothesis between two samples generated from the same population. It has different names such as **exact testing, randomization testing, and re-randomization testing**.  \\n[Article constituant un meilleur exemple imo.]([Tests de permutation avec le logiciel R - DellaData](https://delladata', metadata={'Header 1': 'Part 1, An introduction to statistics', 'Header 2': 'Chapter 2, Distributions of Data', 'source': 'data\\\\computer_science_notes\\\\Building Statistical Models in Python.md'}),\n Document(page_content='### Population inference from samples  \\n- We have to give our study the same degrees of uncertainty as those of the population, to that effect we used **randomized experiments**.\\n- A random experiment has two characteristics:\\n- **random sampling**\\n- **random assignment of treatments**  \\n#### Random assignment of treatments  \\nRefers to two things :\\n- gain an understanding of specific input variables and their influence\\non the response\\n- remove the impact of external variables on the outcomes of a study', metadata={'Header 1': 'Part 1, An introduction to statistics', 'Header 2': 'Chapter 1, Sampling and Generalization', 'source': 'data\\\\computer_science_notes\\\\Building Statistical Models in Python.md'}),\n Document(page_content=\"# Part 1, An introduction to statistics  \\n## Chapter 1, Sampling and Generalization  \\n#### Population versus sample  \\n- The goal of stats modeling is to answer a question about a group by making an inference about that group (the entirety of the group is called a **population**) .\\n- Because it's unlikely to have data on the whole population (can't collect all data, too large),  we use a subset of the population, a **sample**.\\n- This subset needs to be **representative** of the population\", metadata={'Header 1': 'Part 1, An introduction to statistics', 'Header 2': 'Chapter 1, Sampling and Generalization', 'source': 'data\\\\computer_science_notes\\\\Building Statistical Models in Python.md'})]"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"statistical test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our pinecone vector base is working as intended. Relevant documents to our query are correctly emphasised."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "'Based on the provided context, there are four types of probability sampling mentioned:\\n\\n1. **Simple random sampling**: every member has an equal chance.\\n2. **Systematic sampling**: based on a fixed interval, choose a random numbered data point and select the rest of the data along the interval.\\n\\nThese two types of probability sampling are mentioned in the context as part of the \"Probability sampling\" section.'"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an itemgetter allows to create a callable with a set key, and can be used to retrieve related value of a parameter-object with said key,\n",
    "# here the parameter-object is the dict passed through invoke\n",
    "from operator import itemgetter\n",
    "\n",
    "# dict given to the prompt is a Runnable that generates a map with context and question\n",
    "# context comes from our retriever, which receives a 'question' item\n",
    "chain = ({\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "# item is given through invoke when using the chain with our question\n",
    "chain.invoke({\"question\": \"What are the different types of types of probability sampling ?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll fire off a few questions and evaluate our RAG setup."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "How does learning rate affect training ?\n",
      "Answer : \n",
      "Based on the provided context, I can answer that varying the learning rate can impact the training process in the following ways:\n",
      "\n",
      "* If the learning rate is too high, the model may overfit by a larger amount (as seen in the example with two hidden layers).\n",
      "* If the learning rate is too low, the model may not learn as well as when there were no hidden layers.\n",
      "\n",
      "Additionally, it's mentioned that small input values can lead to drastic weight changes, which highlights the importance of choosing an appropriate learning rate.\n",
      "Question: \n",
      "What are some of the different object detection techniques/architectures ?\n",
      "Answer : \n",
      "Based on the provided context, I can identify the following object detection techniques/architectures mentioned:\n",
      "\n",
      "1. R-CNN (Region-based Convolutional Neural Networks)\n",
      "2. Fast R-CNN\n",
      "3. SSD (Single Shot Detector)\n",
      "4. YOLO (You Only Look Once)\n",
      "5. U-Net\n",
      "6. Mask-RCNN\n",
      "7. Detectron2\n",
      "\n",
      "These are some of the modern object detection approaches and architectures mentioned in the context, which include Faster R-CNN architecture and leveraging region proposals generated by selective search.\n",
      "Question: \n",
      "What does ANN stand for ?\n",
      "Answer : \n",
      "Based on the provided context, I can answer that:\n",
      "\n",
      "ANN stands for Artificial Neural Network.\n",
      "Question: \n",
      "Explain cross-entropy and its different forms.\n",
      "Answer : \n",
      "Based on the provided context, I can explain cross-entropy and its different forms.\n",
      "\n",
      "Cross-entropy is a loss function used in machine learning to measure the difference between the predicted output and the actual output. It is commonly used for multi-class classification problems where the target variable has more than two distinct values.\n",
      "\n",
      "There are two main forms of cross-entropy:\n",
      "\n",
      "1. **Categorical Cross-Entropy**: This is used when the target variable has more than two distinct values (i.e., C > 2). The formula for categorical cross-entropy is:\n",
      "\n",
      "![[Pasted image 20230901133354.png]]\n",
      "\n",
      "\n",
      "\n",
      "where y is the actual value of the output, p is the predicted value of the output, m is the total number of data points, and C is the total number of classes.\n",
      "\n",
      "2. **Binary Cross-Entropy**: This is used when the target variable has only two distinct values (i.e., C = 2). The formula for binary cross-entropy is:\n",
      "\n",
      "![[Pasted image 20230901133047.png]]\n",
      "\n",
      "\n",
      "\n",
      "where y is the actual value of the output, p is the predicted value of the output, and m is the total number of data points.\n",
      "\n",
      "In summary, categorical cross-entropy is used for multi-class classification problems with more than two classes, while binary cross-entropy is used for binary classification problems with only two classes.\n",
      "Question: \n",
      "What is the use of a statistical test ?\n",
      "Answer : \n",
      "According to the provided context, the use of a statistical test is \"to help make decisions based on quantifiable uncertainties\".\n",
      "Question: \n",
      "Are there notes relevant to a book in our documents ? Who wrote it ?\n",
      "Answer : \n",
      "Based on the provided context, I can answer your question.\n",
      "\n",
      "Yes, there are notes relevant to a book in our documents. The specific book is \"Modern Computer Vision with PyTorch\" by V Kishore Ayyadevara and Yeshwanth Reddy.\n",
      "\n",
      "The metadata for this document includes tags such as \"book\", \"computer vision\", and \"PyTorch\", indicating that it is related to the book.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"How does learning rate affect training ?\",\n",
    "    \"What are some of the different object detection techniques/architectures ?\",\n",
    "    \"What does ANN stand for ?\",\n",
    "    \"Explain cross-entropy and its different forms.\",\n",
    "    \"What is the use of a statistical test ?\",\n",
    "    \"Are there notes relevant to a book in our documents ? Who wrote it ?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: \\n\\n{question}\")\n",
    "    print(f\"Answer : \\n{chain.invoke({'question': question})}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can still notice the lack of sophistication in our local LLaMa 3's responses. The model tends to rely too much on direct citing, which may be why it references image paths without showcasing the knowledge that they can't be used in this context. As always, model quality is one side of the equation, the other being prompt quality."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can answer that:\n",
      "\n",
      "The project JumperCV is a computer vision project focused on detecting and tracking players in basketball videos. The project involves Multiple Object Tracking (MOT) as its central objective."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({'question': 'What is the project JumperCV ?'}):\n",
    "    print(s, end=\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pleasantly surprised by this one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have successfully integrated a Pinecone vector base into our RAG setup. This allows us to benefit from a faster and less expensive retrieval method, now that we don't have to reload and split our documents every time. This vector base is also easy to expand and displace onto other tools and projects if needed.\n",
    "\n",
    "Next time we'll build upon this base by looking at LangGraph. This tool gives us the capacity to orchestrate an agentic workflow with our LLM chain and refer ourselves to a state graph to do so. We'll try and implement a router that decides if the provided context is valid/helpful, and if not, use function calling to complement knowledge with the internet. We'll monitor all of this through LangSmith, and finally want to test our RAG on an actual frontend through LangServe Chat Playground (this also necessitates conversational memory). In general, we'll keep versioning these notebooks with different experiments in agentic workflows. I will also make these into actual scripts and use them as a module.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
