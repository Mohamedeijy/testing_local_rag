{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing os module for environment variables\n",
    "import os\n",
    "# importing necessary functions from dotenv library\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"llama3:8b\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "\"What a great question!\\n\\nThe color of the sky can appear differently depending on various factors such as time of day, atmospheric conditions, and the observer's location. However, under normal circumstances, the sky typically appears blue to our eyes because of a phenomenon called Rayleigh scattering.\\n\\nRayleigh scattering is the scattering of light by small particles or molecules in the atmosphere, like nitrogen (N2) and oxygen (O2). These gases are present in the air at very high concentrations, making up about 99% of the Earth's atmosphere. When sunlight enters the Earth's atmosphere, it encounters these tiny molecules and scatters in all directions.\\n\\nHere's the key part: shorter (blue) wavelengths scatter more than longer (red) wavelengths. This is because the smaller molecules are more effective at scattering the shorter wavelengths. As a result, the blue light is dispersed throughout the atmosphere, reaching our eyes from all directions.\\n\\nWhen we look up at the sky, we see this scattered blue light, which appears as a brilliant blue color. The color of the sky can vary depending on:\\n\\n1. Time of day: During sunrise and sunset, the sky often takes on hues of orange, red, or pink due to the scattering of longer wavelengths.\\n2. Atmospheric conditions: Dust, pollution, water vapor, and other particles in the air can scatter light, changing the apparent color of the sky.\\n3. Observer's location: The Earth's atmosphere is not uniform; atmospheric conditions can vary significantly depending on the location, altitude, and latitude.\\n\\nSo, to summarize, the sky appears blue because of the scattering of sunlight by small molecules in the Earth's atmosphere, with shorter wavelengths (like blue) being scattered more than longer wavelengths (like red).\\n\\nI hope that helps! Do you have any follow-up questions?\""
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# test openai and llama message response\n",
    "if MODEL.startswith('gpt'):\n",
    "    model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3:8b\")\n",
    "\n",
    "model.invoke(\"Why is the sky blue ?\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenAi gives us an AIMessage object for our OpenAI model, and a string for our local llama model. We use a parser to always dea with strings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\"The sky appears blue because of a phenomenon called Rayleigh scattering, where shorter (blue) wavelengths of light are scattered more than longer (red) wavelengths by tiny molecules of gases like nitrogen and oxygen in the Earth's atmosphere. This scattering effect gives the sky its blue color!\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "# pipe the output of our model into the input of our parser\n",
    "chain = model | parser\n",
    "# instead of invoking the model, we invoke the chain instead\n",
    "chain.invoke(\"Why is the sky blue ? Give a concise answer.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content=\"lang: eng\\nannotation-target: \\nauthor:\\n  - Huy Hoang Nguyen\\n  - Paul N Adams\\n  - Stuart J Miller\\nsubject:\\n  - Statistical Anylisis\\n  - Python\\ntags:\\n  - book\\n  - statisticalmodels\\n\\nOutline\\n\\nAn introduction to statistics\\n\\nRegression models\\n\\nClassification models\\n\\nTime series models\\n\\nSurvival analysis\\n\\nReadthrough\\n\\nPart 1, An introduction to statistics\\n\\nChapter 1, Sampling and Generalization\\n\\nPopulation versus sample\\n\\nThe goal of stats modeling is to answer a question about a group by making an inference about that group (the entirety of the group is called a population) .\\n\\nBecause it's unlikely to have data on the whole population (can't collect all data, too large),  we use a subset of the population, a sample.\\n\\nThis subset needs to be representative of the population.\\n\\nPopulation inference from samples\\n\\nWe have to give our study the same degrees of uncertainty as those of the population, to that effect we used randomized experiments.\\n\\nA random experiment has two characteristics: \\nrandom sampling\\nrandom assignment of treatments\\n\\nRandom assignment of treatments\\n\\nRefers to two things :\\n- gain an understanding of specific input variables and their influence \\non the response\\n- remove the impact of external variables on the outcomes of a study. These external variables are called confounding variables.\\nReferring back to the example in the earlier section, Population versus sample, consider a farmer who decides to start using pesticides on his crops and wants to test two different brands. The farmer knows there are three distinct areas of the land; plot A, plot B, and plot C. To determine the success of the pesticides and prevent damage to the crops, the farmer randomly chooses 60 plants from each plot (this is called stratified random sampling where random sampling is stratified across each plot) for testing. This selection is representative of the population of plants. From this selection, the farmer labels his plants (labeling doesn’t need to be random). For each plot, the farmer shuffles the labels into a bag, to randomize them, and begins selecting 30 plants. The first 30 plants get one of two treatments and the other 30 are given the other treatment. This is a random assignment of treatment. Assuming the three separate plots represent a distinct set of confounding variables on crop yield, the farmer will have enough information to obtain an inference about the crop yield for each pesticide brand.\\n\\nObservational study\\n\\nResearcher learns through observing data but cannot make cause-and-effect conclusion like with a randomized experiment (because it lacks one of the two citeria of it).\\n\\nDone when randomized experiment is not possible (data from random experiment already exists, too costly or just impossible)\\n\\nA way to derive causality from observational studies is through repeated random sampling and repeated analysis.\\n\\nStatistical test\\n\\nStatistics is to help make decisions based on quantifiable uncertainties\\n\\nA hypothesis test contains a null hypothesis (no difference between data) and an alternative hypothesis (difference between data), difference based on a critical value, a benchmark\\n\\nA hypothesis test can test the following:\\nOne variable against another (such as in a t-test)\\nMultiple variables against one variable (for example, linear regression)\\nMultiple variables against multiple variables (for example, MANOVA)\\n\\nSampling strategies\\n\\nTwo types of sampling methods:\\n\\nProbability sampling\\n\\nA sample is chosen based on a theory of probability, or randomly with random selection (every member has the same chance)\\n\\n4 types of probability sampling.\\n\\nSimple random sampling : every member has an equal chance. Used when all member have similar properties related to important features. Minimizes bias and maximizes representativeness.\", metadata={'source': 'building_statistical_models_in_python.md'}),\n Document(page_content='Simple random sampling : every member has an equal chance. Used when all member have similar properties related to important features. Minimizes bias and maximizes representativeness.\\n\\nSystematic sampling :  based on an fixed interval, choose a random numbered data point and select the rest of the data along the interval. Less random than simple random sampling. With systematic sampling, there is a biased risk when the list of members of a population is organized to match the sampling interval.\\nIn a class at one high school in Dallas, there are 50 students but only 10 books to give to these students. The sampling interval is fixed by dividing the number of students in the class by the number of books (50/10 = 5). We also need to generate a random number between one and 50 as a random starting point. For example, take the number 18. Hence, the 10 students selected to get the books will be as follows: 18, 23, 28, 33, 38, 43, 48, 3, 8, 13\\n\\nStratified sampling : based dividing a population a subpopulations called strata. These subpopulations must be distinct so that every member in each stratum has an equal chance of being selected by using simple random sampling. Helps to reduce sample selection bias. On the other hand, when classifying each member of a population into distinct subpopulations is not obvious, this method becomes unusable.\\n\\nCluster sampling : population is divided into clusters with homogenous characteristics. Instead of randomly selecting individual members in each cluster, entire clusters are randomly chosen and each of these clusters has an equal chance of being selected as part of a sample. If clusters are large, then we can conduct a multistage sampling by using one of the previous sampling methods to select individual members within each cluster.\\n\\nNon-probability sampling\\n\\nConvenience sampling : researchers choose members the most accessible to the researchers from a population to form a sample. This method is easy and inexpensive but generalizing the results obtained to the whole population is questionable.\\n\\nQuota sampling : a sample group is selected to be representative of a larger population in a non-random way. For example, recruiters with limited time can use the quota sampling method to search for potential candidates from professional social networks (LinkedIn, Indeed.com, etc.) and interview them. This method is cost-effective and saves time but presents bias during the selection process.\\n\\nChapter 2, Distributions of Data\\n\\nUnderstanding data types\\n\\nNominal data (qualitative)\\n\\nData labeled with distinct groupings (under labels). With nominal data, there is only one operation that can be performed: equality. Each member of a group is equal while members from different groups are unequal.\\n![[Pasted image 20240204165113.png]]\\n\\nOrdinal data (qualitative)\\n\\nNominal data that exhibits an order. University education levels are an example of ordinal data with the levels BS, MS, and PhD. The new operation that is possible here, on top of equality, is ordering, as the data can be sorted.\\n\\nInterval data (quantitative)\\n\\nUsed to describe data that exists on an interval scale but does not have a clear definition of zero. \\nTake the Celsius temperature scale, for example. The data points are numeric, and the data points are evenly spaced at an interval (for example, 20 and 40 are both 10 degrees away from 30). 22 Distributions of Data In this example of the temperature scale, the definition of 0 is arbitrary. For Celsius, 0 happens to be set at water’s freezing point, but this is an arbitrary choice made by the designers of the scale.\\nSo, the interval data type supports equality, ordering, and addition/subtraction.\\n\\nRatio data (quantitative)', metadata={'source': 'building_statistical_models_in_python.md'}),\n Document(page_content=\"Ratio data (quantitative)\\n\\nLike interval data, ratio data is ordered numeric data, but unlike interval data, ratio data has an absolute 0.Absolute 0 means that if the value of a ratio-type variable is zero, none of that variable exists or is present. 0 is the absolute minimum value. Ratio data also supports meaningful multiplication/division, making ratio data the type of data with the most supported operations.\\n\\nMeasuring and describing distributions\\n\\n3 categories of descriptive statistics:\\n- Central tendency\\n- Variability\\n- Shape\\n\\nMeasuring central tendency\\n\\nMode : The most common occurring instance. A dataset can be unimodal or multimodal. In cases of multimodal continuous data, the term is looser, two peaks of a distribution can be called modes even if they don't have the same magnitude.\\n\\nMedian : Middle value occurring when the values occur in order. Can be performed on any data aside from nominal data, as it dosent have order operation. nlike the mean, the median is not affected by outliers if outliers account for a smaller percentage of the data.\\n\\nMean : the average. Valid for interval and ratio data. The mean is pulled toward values with a larger absolute value.\\n\\nMeasuring variability\\n\\nRange : difference between the minimum and the maximum.\\n\\nQuartile ranges :  Range of the quartiles of the distribution after sorting (lower, middle (50%) and higher quartile). The middle quartile range is called the Interquartile Range (IQR).\\n\\nTukey fences : Tukey fences are based on the IQR and defined as follows (k being arbitrary, usually 1.5): \\nLower fence:Q1 − k(IQR) \\nUpper fence:Q3 + k(IQR)\\n\\nVariance : measure of dispersion.\\n\\nMeasuring shape\\n\\nSkewness : The measure of asymmetry. A distribution can be left-skewed (negative skewness value), right-skewed (positive skewness value) or non-skewed (near 0 skewness value).\\n    ![[Pasted image 20240209125831.png]]\\n\\nKurtosis : Measurement of how heavy or light the tail of distribution is relative to normal distribution. If the kurtosis value is 0 or near 0, the distribution does not exhibit kurtosis. If the kurtosis value is negative, the distribution exhibits light-tailedness, and if the kurtosis value is positive, the distribution exhibits heavy-tailedness.\\n    ![[Pasted image 20240209125806.png]]\\n\\nNormal distribution and Central Limit Theorem\\n\\nStandard normal probability distribution : data occurs close more often that further away. = no skew, no kurtosis\\n\\nEmpirical Rule : describes the distrib as having three pertinent standard deviations around the mean μ. It has two assumptions:\\n- The first, second, and third standard deviations contain 68%, 95%, and 99.7% of the measurements dispersed, respectively.\\n- The mean, median, and mode are all equal to each other\\n    ![[Pasted image 20240209130545.png]]\\n\\nTwo common forms of a normal distribution are as follows: \\n- The probability density distribution (based on random sampling)\\n    ![[Pasted image 20240209130737.png]]\\n- The cumulative density distribution (based on accumulative data)\\n    ![[Pasted image 20240209130747.png]]\\n\\nCentral Limit Theorem\\n\\npostulates that if random samples of n observations are taken from a population that has a specific mean, μ, and standard deviation, σ, the sampling distribution constructed from the means of the randomly selected sub-sample distributions will approximate a normal distribution having roughly the same mean, μ, and standard deviation, calculated as ![[Pasted image 20240209131159.png]] as the population.\\n\\n[[Bootstrapping as a demonstration of the CLT]]\\n\\nPermutations\\n\\nBasic knowledge of permutations and combinations\\n\\nThe order of objects matter in permutations while it does not for combinations.\", metadata={'source': 'building_statistical_models_in_python.md'}),\n Document(page_content=\"[[Bootstrapping as a demonstration of the CLT]]\\n\\nPermutations\\n\\nBasic knowledge of permutations and combinations\\n\\nThe order of objects matter in permutations while it does not for combinations.\\n\\nFor exemple, someone needs to choose at random 3 people out 10 to get moneys prices (winners being Huy, Paul, and Stuart). \\nIn one example, he gives out 1000\\\\$, 500\\\\$, et 300\\\\$. In a second, he gives three equal 500$ prices. In the first exemple, the prices are different for each winner so it plays out in more ways than the second exemple as the order prize arrangement doesn't matter, the first example is a permutation example.\\n![[Pasted image 20240209153511.png]]\\nIn Python, the package itertools is used to find permutations directly with permutations.\\n\\nFor the second example, the order doesn't matter. In the first example, when the 3 winners are selected, there are six ways of  arranging the prizes, whereas there is only one way of doing so in the second example.\\n\\n![[Pasted image 20240209154815.png]]\\n\\nIn Python, the package itertools is used to find permutations directly with combinations.\\n\\nPermutation testing\\n\\nWhile bootstrapping is useful for estimating statistical parameters, permutations are useful for hypothesis testing.\\n\\nPermutation testing is used to test the null hypothesis between two samples generated from the same population. It has different names such as exact testing, randomization testing, and re-randomization testing.\\n\\nArticle constituant un meilleur exemple imo.\\n\\nTransformations\\n\\nLog transformation\\n\\nSquare root transformation\\n\\nCube root transformation\\n![[Pasted image 20240209170853.png]]\\n\\nUsing transformation, we can see the transformed histograms are more normally distributed than the original one. It seems that the best transformation in this example is cube root transformation. With real-world data, it is important to determine whether a transformation is needed, and, if so, which transformation should be used.\\n\\nChapter 3, Hypothesis Testing\\n\\nChapter 6, [[Simple Linear Regression]]\\n\\nChapter 7, [[Multiple Linear Regression]]\", metadata={'source': 'building_statistical_models_in_python.md'})]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"data/building_statistical_models_in_python.md\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "langchain_core.documents.base.Document"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a prompt template encompassing the context to give the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. The context\n",
      "may contain sentences in french, interpret them in english.\n",
      "If you can't answer the question, reply 'I don't know'.\n",
      "\n",
      "Context: Here is some context\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "Answer the question based on the context below.\n",
    "If you can't answer the question, reply 'I don't know'.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To pass this prompt to our model, we expand upon our chain."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'PromptInput',\n 'type': 'object',\n 'properties': {'context': {'title': 'Context', 'type': 'string'},\n  'question': {'title': 'Question', 'type': 'string'}}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Based on the context, I would say that according to many people's opinions, including fans and experts in the field, Jimi Hendrix is widely considered to be the greatest guitarist of all time.\""
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to invoke a chain we need to understand the structure of our prompt template, which can be seen on the input schema above\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"In the 1960s, the rock scene was an effervescent field of talented and eccentric musicians brought up through the hippie movement. Jimi Hendrix, Eric Clapton, Jimmy Page are a few guitarists of that era who experienced a lot of success. Jimi Hendrix is still considered to be the best guitarist of all time.\",\n",
    "        \"question\": \"Who is the greatest guitarist ever ?\",\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make it so that our chain receives our documents' relevant information as context, we'll use a very simple vector store that will receive the embeddings generated with our data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A retriever is a component of Langchain that allows to retrieve information from a vector store (can retrieve from other sources). Using invoke to retrieve the top 4 closest documents most relevant to the prompt inputted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content=\"[[Bootstrapping as a demonstration of the CLT]]\\n\\nPermutations\\n\\nBasic knowledge of permutations and combinations\\n\\nThe order of objects matter in permutations while it does not for combinations.\\n\\nFor exemple, someone needs to choose at random 3 people out 10 to get moneys prices (winners being Huy, Paul, and Stuart). \\nIn one example, he gives out 1000\\\\$, 500\\\\$, et 300\\\\$. In a second, he gives three equal 500$ prices. In the first exemple, the prices are different for each winner so it plays out in more ways than the second exemple as the order prize arrangement doesn't matter, the first example is a permutation example.\\n![[Pasted image 20240209153511.png]]\\nIn Python, the package itertools is used to find permutations directly with permutations.\\n\\nFor the second example, the order doesn't matter. In the first example, when the 3 winners are selected, there are six ways of  arranging the prizes, whereas there is only one way of doing so in the second example.\\n\\n![[Pasted image 20240209154815.png]]\\n\\nIn Python, the package itertools is used to find permutations directly with combinations.\\n\\nPermutation testing\\n\\nWhile bootstrapping is useful for estimating statistical parameters, permutations are useful for hypothesis testing.\\n\\nPermutation testing is used to test the null hypothesis between two samples generated from the same population. It has different names such as exact testing, randomization testing, and re-randomization testing.\\n\\nArticle constituant un meilleur exemple imo.\\n\\nTransformations\\n\\nLog transformation\\n\\nSquare root transformation\\n\\nCube root transformation\\n![[Pasted image 20240209170853.png]]\\n\\nUsing transformation, we can see the transformed histograms are more normally distributed than the original one. It seems that the best transformation in this example is cube root transformation. With real-world data, it is important to determine whether a transformation is needed, and, if so, which transformation should be used.\\n\\nChapter 3, Hypothesis Testing\\n\\nChapter 6, [[Simple Linear Regression]]\\n\\nChapter 7, [[Multiple Linear Regression]]\", metadata={'source': 'building_statistical_models_in_python.md'}),\n Document(page_content=\"lang: eng\\nannotation-target: \\nauthor:\\n  - Huy Hoang Nguyen\\n  - Paul N Adams\\n  - Stuart J Miller\\nsubject:\\n  - Statistical Anylisis\\n  - Python\\ntags:\\n  - book\\n  - statisticalmodels\\n\\nOutline\\n\\nAn introduction to statistics\\n\\nRegression models\\n\\nClassification models\\n\\nTime series models\\n\\nSurvival analysis\\n\\nReadthrough\\n\\nPart 1, An introduction to statistics\\n\\nChapter 1, Sampling and Generalization\\n\\nPopulation versus sample\\n\\nThe goal of stats modeling is to answer a question about a group by making an inference about that group (the entirety of the group is called a population) .\\n\\nBecause it's unlikely to have data on the whole population (can't collect all data, too large),  we use a subset of the population, a sample.\\n\\nThis subset needs to be representative of the population.\\n\\nPopulation inference from samples\\n\\nWe have to give our study the same degrees of uncertainty as those of the population, to that effect we used randomized experiments.\\n\\nA random experiment has two characteristics: \\nrandom sampling\\nrandom assignment of treatments\\n\\nRandom assignment of treatments\\n\\nRefers to two things :\\n- gain an understanding of specific input variables and their influence \\non the response\\n- remove the impact of external variables on the outcomes of a study. These external variables are called confounding variables.\\nReferring back to the example in the earlier section, Population versus sample, consider a farmer who decides to start using pesticides on his crops and wants to test two different brands. The farmer knows there are three distinct areas of the land; plot A, plot B, and plot C. To determine the success of the pesticides and prevent damage to the crops, the farmer randomly chooses 60 plants from each plot (this is called stratified random sampling where random sampling is stratified across each plot) for testing. This selection is representative of the population of plants. From this selection, the farmer labels his plants (labeling doesn’t need to be random). For each plot, the farmer shuffles the labels into a bag, to randomize them, and begins selecting 30 plants. The first 30 plants get one of two treatments and the other 30 are given the other treatment. This is a random assignment of treatment. Assuming the three separate plots represent a distinct set of confounding variables on crop yield, the farmer will have enough information to obtain an inference about the crop yield for each pesticide brand.\\n\\nObservational study\\n\\nResearcher learns through observing data but cannot make cause-and-effect conclusion like with a randomized experiment (because it lacks one of the two citeria of it).\\n\\nDone when randomized experiment is not possible (data from random experiment already exists, too costly or just impossible)\\n\\nA way to derive causality from observational studies is through repeated random sampling and repeated analysis.\\n\\nStatistical test\\n\\nStatistics is to help make decisions based on quantifiable uncertainties\\n\\nA hypothesis test contains a null hypothesis (no difference between data) and an alternative hypothesis (difference between data), difference based on a critical value, a benchmark\\n\\nA hypothesis test can test the following:\\nOne variable against another (such as in a t-test)\\nMultiple variables against one variable (for example, linear regression)\\nMultiple variables against multiple variables (for example, MANOVA)\\n\\nSampling strategies\\n\\nTwo types of sampling methods:\\n\\nProbability sampling\\n\\nA sample is chosen based on a theory of probability, or randomly with random selection (every member has the same chance)\\n\\n4 types of probability sampling.\\n\\nSimple random sampling : every member has an equal chance. Used when all member have similar properties related to important features. Minimizes bias and maximizes representativeness.\", metadata={'source': 'building_statistical_models_in_python.md'}),\n Document(page_content='Simple random sampling : every member has an equal chance. Used when all member have similar properties related to important features. Minimizes bias and maximizes representativeness.\\n\\nSystematic sampling :  based on an fixed interval, choose a random numbered data point and select the rest of the data along the interval. Less random than simple random sampling. With systematic sampling, there is a biased risk when the list of members of a population is organized to match the sampling interval.\\nIn a class at one high school in Dallas, there are 50 students but only 10 books to give to these students. The sampling interval is fixed by dividing the number of students in the class by the number of books (50/10 = 5). We also need to generate a random number between one and 50 as a random starting point. For example, take the number 18. Hence, the 10 students selected to get the books will be as follows: 18, 23, 28, 33, 38, 43, 48, 3, 8, 13\\n\\nStratified sampling : based dividing a population a subpopulations called strata. These subpopulations must be distinct so that every member in each stratum has an equal chance of being selected by using simple random sampling. Helps to reduce sample selection bias. On the other hand, when classifying each member of a population into distinct subpopulations is not obvious, this method becomes unusable.\\n\\nCluster sampling : population is divided into clusters with homogenous characteristics. Instead of randomly selecting individual members in each cluster, entire clusters are randomly chosen and each of these clusters has an equal chance of being selected as part of a sample. If clusters are large, then we can conduct a multistage sampling by using one of the previous sampling methods to select individual members within each cluster.\\n\\nNon-probability sampling\\n\\nConvenience sampling : researchers choose members the most accessible to the researchers from a population to form a sample. This method is easy and inexpensive but generalizing the results obtained to the whole population is questionable.\\n\\nQuota sampling : a sample group is selected to be representative of a larger population in a non-random way. For example, recruiters with limited time can use the quota sampling method to search for potential candidates from professional social networks (LinkedIn, Indeed.com, etc.) and interview them. This method is cost-effective and saves time but presents bias during the selection process.\\n\\nChapter 2, Distributions of Data\\n\\nUnderstanding data types\\n\\nNominal data (qualitative)\\n\\nData labeled with distinct groupings (under labels). With nominal data, there is only one operation that can be performed: equality. Each member of a group is equal while members from different groups are unequal.\\n![[Pasted image 20240204165113.png]]\\n\\nOrdinal data (qualitative)\\n\\nNominal data that exhibits an order. University education levels are an example of ordinal data with the levels BS, MS, and PhD. The new operation that is possible here, on top of equality, is ordering, as the data can be sorted.\\n\\nInterval data (quantitative)\\n\\nUsed to describe data that exists on an interval scale but does not have a clear definition of zero. \\nTake the Celsius temperature scale, for example. The data points are numeric, and the data points are evenly spaced at an interval (for example, 20 and 40 are both 10 degrees away from 30). 22 Distributions of Data In this example of the temperature scale, the definition of 0 is arbitrary. For Celsius, 0 happens to be set at water’s freezing point, but this is an arbitrary choice made by the designers of the scale.\\nSo, the interval data type supports equality, ordering, and addition/subtraction.\\n\\nRatio data (quantitative)', metadata={'source': 'building_statistical_models_in_python.md'}),\n Document(page_content=\"Ratio data (quantitative)\\n\\nLike interval data, ratio data is ordered numeric data, but unlike interval data, ratio data has an absolute 0.Absolute 0 means that if the value of a ratio-type variable is zero, none of that variable exists or is present. 0 is the absolute minimum value. Ratio data also supports meaningful multiplication/division, making ratio data the type of data with the most supported operations.\\n\\nMeasuring and describing distributions\\n\\n3 categories of descriptive statistics:\\n- Central tendency\\n- Variability\\n- Shape\\n\\nMeasuring central tendency\\n\\nMode : The most common occurring instance. A dataset can be unimodal or multimodal. In cases of multimodal continuous data, the term is looser, two peaks of a distribution can be called modes even if they don't have the same magnitude.\\n\\nMedian : Middle value occurring when the values occur in order. Can be performed on any data aside from nominal data, as it dosent have order operation. nlike the mean, the median is not affected by outliers if outliers account for a smaller percentage of the data.\\n\\nMean : the average. Valid for interval and ratio data. The mean is pulled toward values with a larger absolute value.\\n\\nMeasuring variability\\n\\nRange : difference between the minimum and the maximum.\\n\\nQuartile ranges :  Range of the quartiles of the distribution after sorting (lower, middle (50%) and higher quartile). The middle quartile range is called the Interquartile Range (IQR).\\n\\nTukey fences : Tukey fences are based on the IQR and defined as follows (k being arbitrary, usually 1.5): \\nLower fence:Q1 − k(IQR) \\nUpper fence:Q3 + k(IQR)\\n\\nVariance : measure of dispersion.\\n\\nMeasuring shape\\n\\nSkewness : The measure of asymmetry. A distribution can be left-skewed (negative skewness value), right-skewed (positive skewness value) or non-skewed (near 0 skewness value).\\n    ![[Pasted image 20240209125831.png]]\\n\\nKurtosis : Measurement of how heavy or light the tail of distribution is relative to normal distribution. If the kurtosis value is 0 or near 0, the distribution does not exhibit kurtosis. If the kurtosis value is negative, the distribution exhibits light-tailedness, and if the kurtosis value is positive, the distribution exhibits heavy-tailedness.\\n    ![[Pasted image 20240209125806.png]]\\n\\nNormal distribution and Central Limit Theorem\\n\\nStandard normal probability distribution : data occurs close more often that further away. = no skew, no kurtosis\\n\\nEmpirical Rule : describes the distrib as having three pertinent standard deviations around the mean μ. It has two assumptions:\\n- The first, second, and third standard deviations contain 68%, 95%, and 99.7% of the measurements dispersed, respectively.\\n- The mean, median, and mode are all equal to each other\\n    ![[Pasted image 20240209130545.png]]\\n\\nTwo common forms of a normal distribution are as follows: \\n- The probability density distribution (based on random sampling)\\n    ![[Pasted image 20240209130737.png]]\\n- The cumulative density distribution (based on accumulative data)\\n    ![[Pasted image 20240209130747.png]]\\n\\nCentral Limit Theorem\\n\\npostulates that if random samples of n observations are taken from a population that has a specific mean, μ, and standard deviation, σ, the sampling distribution constructed from the means of the randomly selected sub-sample distributions will approximate a normal distribution having roughly the same mean, μ, and standard deviation, calculated as ![[Pasted image 20240209131159.png]] as the population.\\n\\n[[Bootstrapping as a demonstration of the CLT]]\\n\\nPermutations\\n\\nBasic knowledge of permutations and combinations\\n\\nThe order of objects matter in permutations while it does not for combinations.\", metadata={'source': 'building_statistical_models_in_python.md'})]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"statistical test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The retriever allows us to generate context that can be passed within our chain. The prompt object expects a map so we need to integrate our retriver with that in mind."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "'According to the provided context, a statistical test is used \"to help make decisions based on quantifiable uncertainties\". It also contains a null hypothesis (no difference between data) and an alternative hypothesis (difference between data), with a critical value or benchmark. Additionally, it can be used to:\\n\\n* Test one variable against another (e.g., t-test)\\n* Test multiple variables against one variable (e.g., linear regression)\\n* Test multiple variables against multiple variables (e.g., MANOVA)'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an itemgetter allows to create a callable with a set key, and can be used to retrieve related value of a parameter-object with said key,\n",
    "# here the parameter-object is the dict passed through invoke\n",
    "from operator import itemgetter\n",
    "\n",
    "# dict given to the prompt is a Runnable that generates a map with context and question\n",
    "# context comes from our retriever, which receives a 'question' item\n",
    "chain = ({\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "# item is given through invoke when using the chain with our question\n",
    "chain.invoke({\"question\": \"What is a statistical test used for?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the retriever gave the pieces of text related to our question (statistical tests) to the mapped object which completed our formatted prompt. The model answers with phrasing used in our Markdown document and cites examples directly taken from it.\n",
    "We now test our chain with a series of question to pin down its effectiveness."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "Who are the authors of the book ?\n",
      "Answer : \n",
      "According to the context, the authors of the book are:\n",
      "\n",
      "* Huy Hoang Nguyen\n",
      "* Paul N Adams\n",
      "* Stuart J Miller\n",
      "Question: \n",
      "What are the main subjects of the book ?\n",
      "Answer : \n",
      "Based on the context, the main subjects of the book are:\n",
      "\n",
      "1. Statistical Analysis\n",
      "2. Python\n",
      "3. Regression models\n",
      "4. Classification models\n",
      "5. Time series models\n",
      "6. Survival analysis\n",
      "\n",
      "These subjects are listed under the \"Outline\" section at the beginning of the book, which suggests that they are the primary topics covered in the book.\n",
      "Question: \n",
      "What are the different ways to evaluate distributions ?\n",
      "Answer : \n",
      "Based on the provided context, there is no specific information about evaluating distributions. However, I can provide a general overview of common methods used to evaluate or understand the distribution of data:\n",
      "\n",
      "1. **Summary Statistics**: Calculating mean, median, mode, standard deviation, variance, and other measures to get an overall sense of the distribution.\n",
      "2. **Visualization**: Plotting histograms, box plots, density plots, scatter plots, etc., to visualize the shape and characteristics of the distribution.\n",
      "3. **Moments**: Calculating moments (mean, variance, skewness) to quantify the distribution's properties.\n",
      "4. **Fitted Distributions**: Fitting different distributions (e.g., normal, Poisson, exponential) to the data using maximum likelihood estimation or other methods.\n",
      "5. **Goodness-of-Fit Tests**: Performing statistical tests (e.g., chi-squared, Kolmogorov-Smirnov) to determine whether the observed distribution matches a theoretical distribution.\n",
      "\n",
      "These are just some common methods used to evaluate distributions. If you have specific requirements or questions about evaluating distributions for your problem, I'd be happy to help.\n",
      "Question: \n",
      "How to sample data ?\n",
      "Answer : \n",
      "According to the context, there are two types of sampling methods:\n",
      "\n",
      "1. **Probability sampling**: A sample is chosen based on a theory of probability, or randomly with random selection (every member has the same chance). This includes:\n",
      "\t* Simple Random Sampling: Every member has an equal chance.\n",
      "\t* Systematic Sampling: Based on a fixed interval, choose a random numbered data point and select the rest of the data along the interval.\n",
      "\t* Stratified Sampling: Divide a population into subpopulations called strata. These subpopulations must be distinct so that every member in each stratum has an equal chance of being selected by using simple random sampling.\n",
      "\t* Cluster Sampling: Population is divided into clusters with homogeneous characteristics. Instead of randomly selecting individual members in each cluster, entire clusters are randomly chosen and each of these clusters has an equal chance of being selected as part of a sample.\n",
      "2. **Non-Probability Sampling**: A sample is not chosen based on probability or random selection. This includes:\n",
      "\t* Convenience Sampling: Researchers choose members that are most accessible to the researchers from a population to form a sample.\n",
      "\t* Quota Sampling: A sample group is selected to be representative of a larger population in a non-random way.\n",
      "\n",
      "It seems that the context is discussing sampling methods for observational studies, which can help make decisions based on quantifiable uncertainties.\n",
      "Question: \n",
      "How much does the book cost ?\n",
      "Answer : \n",
      "The context provided is about statistical concepts such as observational studies, randomized experiments, and sampling strategies. It also describes different types of data (nominal, ordinal, interval, ratio) and their characteristics.\n",
      "\n",
      "However, there is no information provided about the cost of a book in this context. Therefore, it's not possible to answer how much the book costs based on the given information.\n",
      "Question: \n",
      "What are the topics that will be covered by the document ?\n",
      "Answer : \n",
      "Based on the provided context, it appears that the document will cover various topics related to statistics and statistical modeling in Python. Specifically, the topics include:\n",
      "\n",
      "1. An introduction to statistics\n",
      "2. Regression models\n",
      "3. Classification models\n",
      "4. Time series models\n",
      "5. Survival analysis\n",
      "6. Readthrough (likely an introduction or overview)\n",
      "7. Part 1: An Introduction to Statistics\n",
      "8. Chapter 1: Sampling and Generalization\n",
      "\n",
      "These topics seem to be related to statistical concepts, such as sampling, hypothesis testing, and statistical inference, which are common in data science and machine learning applications.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Who are the authors of the book ?\",\n",
    "    \"What are the main subjects of the book ?\",\n",
    "    \"What are the different ways to evaluate distributions ?\",\n",
    "    \"How to sample data ?\",\n",
    "    \"How much does the book cost ?\",\n",
    "    \"What are the topics that will be covered by the document ?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: \\n{question}\")\n",
    "    print(f\"Answer : \\n{chain.invoke({'question': question})}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, llama3 is not good as at consistent retrieval and summarization, much slower and much more verbose in its answers compared to GPT 3.5."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of printing in one shot at `invoke`, we can use `stream`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, it seems that permutation testing is not explicitly mentioned. However, since we are discussing randomization and statistical tests, I'll try to provide an answer based on related concepts.\n",
      "\n",
      "Permutation testing is a type of non-parametric statistical test that is used to determine whether there is a significant association between two variables or not. It's particularly useful when the data does not follow a normal distribution or when the assumptions of parametric tests are not met. Permutation testing can be used for both continuous and categorical outcomes.\n",
      "\n",
      "In the context of the problem, permutation testing could be used to test whether there is a significant difference in crop yield between different pesticide brands. This would involve randomly permuting the treatment labels and recalculating the statistic (e.g., mean difference) multiple times to generate a null distribution under the hypothesis that there is no difference between treatments. The actual value of the statistic could then be compared to this null distribution to determine whether it's significantly different, indicating a real effect of pesticide brand on crop yield.\n",
      "\n",
      "Please note that this answer assumes a certain level of knowledge about statistical testing and permutation testing. If you'd like me to elaborate or provide more context, feel free to ask!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({'question': 'What is permutation testing useful for ?'}):\n",
    "    print(s, end=\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Again, failure to infer the correct response.)\n",
    "We can also batch the answers, get them all at once."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "['Based on the provided context, the authors of the book \"Building Statistical Models in Python\" are:\\n\\n1. Huy Hoang Nguyen\\n2. Paul N Adams\\n3. Stuart J Miller',\n 'Based on the context, the main subjects of the book \"Building Statistical Models in Python\" appear to be:\\n\\n1. **Statistical Analysis**: The book covers various statistical concepts and techniques, including hypothesis testing, regression models, classification models, time series models, survival analysis, and more.\\n2. **Python**: The book focuses on building statistical models using Python, which implies that it will cover various Python libraries and tools relevant to statistical analysis.\\n\\nThese are the primary subjects of the book, as inferred from the provided context.',\n 'Based on the provided context, it appears that you are referring to a discussion of statistical methods for evaluating data distributions.\\n\\nIn this context, \"evaluating distributions\" likely refers to understanding and analyzing the characteristics of a dataset\\'s underlying distribution (or pattern). Here are some ways to evaluate distributions:\\n\\n1. **Visual inspection**: Plotting the data using various visualization techniques such as histograms, box plots, or scatter plots can help identify patterns, skewness, outliers, and other features of the distribution.\\n2. **Summary statistics**: Calculating measures like mean, median, mode, standard deviation, variance, and percentiles (e.g., quartiles) can provide insight into the central tendency and variability of the data.\\n3. **Moment-based statistics**: Moments, such as skewness and kurtosis, can be used to characterize the shape of a distribution.\\n4. **Model-based approaches**: Fitting statistical models, like normal or t-distributions, to the data can help evaluate whether the observed distribution is consistent with a specific model.\\n\\nThese methods can aid in understanding the properties of a dataset\\'s underlying distribution, which is essential for making informed decisions and drawing valid conclusions from the data.']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"question\": question} for question in questions[:3]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next time we'll use a Pinecone index to have a static and more organized retrieval and embedding search method. We'll also experiment in different ways:\n",
    " - implement function calling like websearch,\n",
    "  - orchestration of an agent workflow with LangGraph to:\n",
    "    - route prompts for different retrieval methods - document/RAG retrieval or websearch for example,\n",
    "    - have a fallback mechanism to progress with when the context retrieved is irrelevant,\n",
    "    - evaluate response with a reflection system or hallucination metric;\n",
    " - integration of this functionality in streamlit, plugins or other.\n",
    " - another vector database that is self-hosted (Qdrant or Milvus) or different data structures like graphs or entity databases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}